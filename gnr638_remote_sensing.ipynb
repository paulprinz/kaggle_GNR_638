{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b951e24",
   "metadata": {},
   "source": [
    "# GNR 638: Machine Learning for Remote Sensing\n",
    "\n",
    "**Task:** Multi-class image classification of remotely sensed images  \n",
    "**Competition:** [gnr638-mls4rs-a1 on Kaggle](https://www.kaggle.com/competitions/gnr638-mls4rs-a1)  \n",
    "**Data source:** [PatternNet](https://huggingface.co/datasets/blanchon/PatternNet) (HuggingFace) — publicly available benchmark that matches the competition class set  \n",
    "**Architecture:** ResNet-50 with ImageNet pre-training (transfer learning)  \n",
    "**Classes (7):** Basketball Court, Beach, Forest, Railway, Swimming Pool, Tennis Court, Others\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "The Kaggle competition (GNR 638, IIT Bombay) is closed and invitation-only, so the original data is inaccessible.\n",
    "**PatternNet** is a widely-used public remote sensing benchmark with 38 scene classes (800 images × 256×256 px each).\n",
    "Six of its classes map directly onto the competition classes; all remaining 32 classes are pooled into an *Others* category.\n",
    "\n",
    "| Competition class | PatternNet class | PatternNet index |\n",
    "|---|---|---|\n",
    "| Basketball Court | `basketball_court` | 2 |\n",
    "| Beach | `beach` | 3 |\n",
    "| Forest | `forest` | 14 |\n",
    "| Railway | `railway` | 26 |\n",
    "| Swimming Pool (Water Pool) | `swimming_pool` | 34 |\n",
    "| Tennis Court | `tennis_court` | 35 |\n",
    "| Others | remaining 32 classes (subsampled) | — |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8312b15d",
   "metadata": {},
   "source": [
    "## 0. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da929595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from pathlib import Path\n",
    "\n",
    "# ── Reproducibility ──────────────────────────────────────────────────────────\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'PyTorch {torch.__version__} | Device: {DEVICE}')\n",
    "if DEVICE.type == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)} | '\n",
    "          f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373bdd4e",
   "metadata": {},
   "source": [
    "## 1. Data Download — PatternNet via HuggingFace\n",
    "\n",
    "PatternNet is downloaded automatically by the HuggingFace `datasets` library and cached locally.\n",
    "No account or API key is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4a564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print('Downloading PatternNet from HuggingFace (cached after first run) ...')\n",
    "hf_dataset = load_dataset('blanchon/PatternNet', split='train')\n",
    "\n",
    "print(f'\\nDataset loaded.')\n",
    "print(f'  Total samples : {len(hf_dataset)}')\n",
    "print(f'  Columns       : {hf_dataset.column_names}')\n",
    "print(f'  Image type    : {type(hf_dataset[0][\"image\"])}')\n",
    "print(f'  Image size    : {hf_dataset[0][\"image\"].size}  (W x H)')\n",
    "print(f'  Num classes   : {hf_dataset.features[\"label\"].num_classes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b97acb6",
   "metadata": {},
   "source": [
    "## 2. Class Mapping & Dataset Construction\n",
    "\n",
    "Map PatternNet's 38 classes to our 7 competition classes.  \n",
    "The *Others* category is subsampled to 800 images to maintain class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85239350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── PatternNet label names ────────────────────────────────────────────────────\n",
    "PN_LABEL_NAMES = hf_dataset.features['label'].names\n",
    "print('PatternNet classes:')\n",
    "for i, name in enumerate(PN_LABEL_NAMES):\n",
    "    print(f'  {i:>2}  {name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd23ec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Competition class definitions ─────────────────────────────────────────────\n",
    "CLASS_NAMES = [\n",
    "    'basketball_court',\n",
    "    'beach',\n",
    "    'forest',\n",
    "    'railway',\n",
    "    'swimming_pool',\n",
    "    'tennis_court',\n",
    "    'others',\n",
    "]\n",
    "CLASS_TO_IDX = {name: i for i, name in enumerate(CLASS_NAMES)}\n",
    "NUM_CLASSES  = len(CLASS_NAMES)\n",
    "\n",
    "# PatternNet index → our competition label index\n",
    "# Any PatternNet index not listed here maps to 'others' (6)\n",
    "PN_TO_COMP = {\n",
    "    PN_LABEL_NAMES.index('basketball_court'): CLASS_TO_IDX['basketball_court'],\n",
    "    PN_LABEL_NAMES.index('beach'):            CLASS_TO_IDX['beach'],\n",
    "    PN_LABEL_NAMES.index('forest'):           CLASS_TO_IDX['forest'],\n",
    "    PN_LABEL_NAMES.index('railway'):          CLASS_TO_IDX['railway'],\n",
    "    PN_LABEL_NAMES.index('swimming_pool'):    CLASS_TO_IDX['swimming_pool'],\n",
    "    PN_LABEL_NAMES.index('tennis_court'):     CLASS_TO_IDX['tennis_court'],\n",
    "}\n",
    "OTHERS_LABEL = CLASS_TO_IDX['others']   # = 6\n",
    "\n",
    "print(f'Competition classes ({NUM_CLASSES}):', CLASS_NAMES)\n",
    "print(f'\\nPatternNet index → competition label:')\n",
    "for pn_idx, comp_idx in PN_TO_COMP.items():\n",
    "    print(f'  PN[{pn_idx:>2}] {PN_LABEL_NAMES[pn_idx]:<22} → [{comp_idx}] {CLASS_NAMES[comp_idx]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12617f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Partition all 30,400 indices by competition label ─────────────────────────\n",
    "OTHERS_SAMPLE = 800   # subsample 'others' to match per-class count\n",
    "\n",
    "indices_by_class = {i: [] for i in range(NUM_CLASSES)}\n",
    "pn_labels = hf_dataset['label']   # list of int, fast access\n",
    "\n",
    "for idx, pn_lbl in enumerate(pn_labels):\n",
    "    comp_lbl = PN_TO_COMP.get(pn_lbl, OTHERS_LABEL)\n",
    "    indices_by_class[comp_lbl].append(idx)\n",
    "\n",
    "# Subsample 'others'\n",
    "rng = random.Random(SEED)\n",
    "indices_by_class[OTHERS_LABEL] = rng.sample(\n",
    "    indices_by_class[OTHERS_LABEL], OTHERS_SAMPLE\n",
    ")\n",
    "\n",
    "print('Images per competition class (after subsampling others):')\n",
    "total = 0\n",
    "counts = {}\n",
    "for i, name in enumerate(CLASS_NAMES):\n",
    "    n = len(indices_by_class[i])\n",
    "    counts[name] = n\n",
    "    total += n\n",
    "    print(f'  [{i}] {name:<22} {n}')\n",
    "print(f'  {\"TOTAL\":<26} {total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8216ec85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Stratified train / val / test split (70 / 15 / 15) ───────────────────────\n",
    "TRAIN_FRAC = 0.70\n",
    "VAL_FRAC   = 0.15\n",
    "# TEST_FRAC = 0.15 (remainder)\n",
    "\n",
    "train_indices, val_indices, test_indices = [], [], []\n",
    "\n",
    "for cls_idx in range(NUM_CLASSES):\n",
    "    idxs = indices_by_class[cls_idx].copy()\n",
    "    rng.shuffle(idxs)\n",
    "    n       = len(idxs)\n",
    "    n_train = int(n * TRAIN_FRAC)\n",
    "    n_val   = int(n * VAL_FRAC)\n",
    "    train_indices.extend(idxs[:n_train])\n",
    "    val_indices.extend(idxs[n_train:n_train + n_val])\n",
    "    test_indices.extend(idxs[n_train + n_val:])\n",
    "\n",
    "print(f'Split sizes  — train: {len(train_indices)}, '\n",
    "      f'val: {len(val_indices)}, test: {len(test_indices)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abd3640",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb68757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Class distribution bar chart ─────────────────────────────────────────────\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "bars = ax.bar(counts.keys(), counts.values(), color='steelblue', edgecolor='white')\n",
    "ax.set_title('Dataset — Class Distribution (after subsampling Others)', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Class', fontsize=11)\n",
    "ax.set_ylabel('Number of Images', fontsize=11)\n",
    "ax.tick_params(axis='x', rotation=25)\n",
    "for bar, val in zip(bars, counts.values()):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 2,\n",
    "            str(val), ha='center', va='bottom', fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.savefig('class_distribution.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Class balance ratio (min/max): {min(counts.values())/max(counts.values()):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175b23eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Sample images per class ───────────────────────────────────────────────────\n",
    "SAMPLES_PER_CLASS = 3\n",
    "\n",
    "# Build a quick lookup: comp_label → list of HF indices (from full set)\n",
    "fig, axes = plt.subplots(NUM_CLASSES, SAMPLES_PER_CLASS,\n",
    "                         figsize=(SAMPLES_PER_CLASS * 3, NUM_CLASSES * 3))\n",
    "fig.suptitle('Sample Images per Class (PatternNet)', fontsize=13, fontweight='bold', y=1.01)\n",
    "\n",
    "for row, cls_name in enumerate(CLASS_NAMES):\n",
    "    cls_idx  = CLASS_TO_IDX[cls_name]\n",
    "    pool     = indices_by_class[cls_idx]\n",
    "    samples  = rng.sample(pool, min(SAMPLES_PER_CLASS, len(pool)))\n",
    "    for col, hf_idx in enumerate(samples):\n",
    "        ax  = axes[row][col]\n",
    "        img = hf_dataset[hf_idx]['image']   # PIL image\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        if col == 0:\n",
    "            ax.set_ylabel(cls_name.replace('_', ' ').title(),\n",
    "                          fontsize=8, rotation=0, labelpad=70, va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sample_images.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140a7c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Image size statistics ─────────────────────────────────────────────────────\n",
    "# PatternNet images are uniformly 256x256; confirm on a sample\n",
    "STAT_SAMPLE = 200\n",
    "sample_idxs = rng.sample(range(len(hf_dataset)), STAT_SAMPLE)\n",
    "widths  = [hf_dataset[i]['image'].size[0] for i in sample_idxs]\n",
    "heights = [hf_dataset[i]['image'].size[1] for i in sample_idxs]\n",
    "\n",
    "print(f'Image dimensions (W x H) — sample of {STAT_SAMPLE}:')\n",
    "print(f'  Width  — min: {min(widths)}, max: {max(widths)}, mean: {np.mean(widths):.0f}')\n",
    "print(f'  Height — min: {min(heights)}, max: {max(heights)}, mean: {np.mean(heights):.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36512c56",
   "metadata": {},
   "source": [
    "## 4. Data Augmentation & DataLoaders\n",
    "\n",
    "ImageNet normalisation statistics are used since ResNet-50 was pre-trained on ImageNet.  \n",
    "Training transforms include random crop, flips, rotations, colour jitter, and random erasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634b5401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Hyperparameters ───────────────────────────────────────────────────────────\n",
    "IMG_SIZE    = 224\n",
    "BATCH_SIZE  = 32\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE + 32, IMG_SIZE + 32)),\n",
    "    transforms.RandomCrop(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.3),\n",
    "    transforms.RandomRotation(degrees=20),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.05),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "    transforms.RandomErasing(p=0.2, scale=(0.02, 0.1)),\n",
    "])\n",
    "\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "print(f'Input size : {IMG_SIZE}x{IMG_SIZE} | Batch size: {BATCH_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edebb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatternNetDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Wraps a HuggingFace PatternNet split with competition label mapping\n",
    "    and optional torchvision transform.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hf_dataset, indices, pn_to_comp, others_label, transform=None):\n",
    "        self.hf_dataset   = hf_dataset\n",
    "        self.indices      = indices          # HF dataset indices to expose\n",
    "        self.pn_to_comp   = pn_to_comp       # {pn_int: comp_int}\n",
    "        self.others_label = others_label\n",
    "        self.transform    = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        hf_idx  = self.indices[idx]\n",
    "        item    = self.hf_dataset[hf_idx]\n",
    "        image   = item['image'].convert('RGB')      # PIL image\n",
    "        pn_lbl  = item['label']\n",
    "        comp_lbl = self.pn_to_comp.get(pn_lbl, self.others_label)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, comp_lbl\n",
    "\n",
    "\n",
    "train_ds = PatternNetDataset(hf_dataset, train_indices, PN_TO_COMP, OTHERS_LABEL, train_transform)\n",
    "val_ds   = PatternNetDataset(hf_dataset, val_indices,   PN_TO_COMP, OTHERS_LABEL, eval_transform)\n",
    "test_ds  = PatternNetDataset(hf_dataset, test_indices,  PN_TO_COMP, OTHERS_LABEL, eval_transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(f'Train : {len(train_ds):>5} images  |  {len(train_loader):>3} batches')\n",
    "print(f'Val   : {len(val_ds):>5} images  |  {len(val_loader):>3} batches')\n",
    "print(f'Test  : {len(test_ds):>5} images  |  {len(test_loader):>3} batches')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be62c878",
   "metadata": {},
   "source": [
    "## 5. Model — ResNet-50 with Fine-Tuning\n",
    "\n",
    "Load ImageNet pre-trained ResNet-50, replace the final FC layer with a dropout + linear head for 7-class output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade80e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet50(num_classes: int, pretrained: bool = True):\n",
    "    \"\"\"Build ResNet-50 with a custom classification head.\"\"\"\n",
    "    weights = models.ResNet50_Weights.IMAGENET1K_V2 if pretrained else None\n",
    "    model   = models.resnet50(weights=weights)\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(p=0.4),\n",
    "        nn.Linear(in_features, num_classes)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_resnet50(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "total_params     = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total parameters     : {total_params:,}')\n",
    "print(f'Trainable parameters : {trainable_params:,}')\n",
    "print(f'Classification head  : {model.fc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc401f2",
   "metadata": {},
   "source": [
    "## 6. Training\n",
    "\n",
    "- **Loss:** Cross-entropy with label smoothing (0.1)  \n",
    "- **Optimiser:** AdamW with weight decay  \n",
    "- **Scheduler:** Cosine annealing with warm restarts  \n",
    "- **Early stopping:** Patience of 7 epochs on validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ced507",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS   = 30\n",
    "LR           = 3e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "PATIENCE     = 7\n",
    "CKPT_DIR     = Path('checkpoints')\n",
    "CKPT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1)\n",
    "\n",
    "print(f'Epochs: {NUM_EPOCHS} | LR: {LR} | Weight decay: {WEIGHT_DECAY} | Patience: {PATIENCE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb111ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress_bar(current, total, length=40):\n",
    "    \"\"\"Print a simple progress bar that updates in place.\"\"\"\n",
    "    progress = current / total\n",
    "    filled   = int(length * progress)\n",
    "    bar      = '#' * filled + '.' * (length - filled)\n",
    "    print(f'\\rProgress: [{bar}] {current}/{total} ({progress*100:.1f}%)',\n",
    "          end='', flush=True)\n",
    "    if current == total:\n",
    "        print()\n",
    "\n",
    "\n",
    "def run_epoch(model, loader, criterion, optimizer=None):\n",
    "    \"\"\"Run one epoch. Pass optimizer=None for evaluation mode.\"\"\"\n",
    "    training = optimizer is not None\n",
    "    model.train() if training else model.eval()\n",
    "    running_loss = correct = total = 0\n",
    "\n",
    "    with torch.set_grad_enabled(training):\n",
    "        for images, labels in loader:\n",
    "            images = images.to(DEVICE, non_blocking=True)\n",
    "            labels = labels.to(DEVICE, non_blocking=True)\n",
    "            outputs = model(images)\n",
    "            loss    = criterion(outputs, labels)\n",
    "            if training:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            correct      += (outputs.argmax(1) == labels).sum().item()\n",
    "            total        += images.size(0)\n",
    "\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "\n",
    "print('Training utilities defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa70a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "best_val_acc  = 0.0\n",
    "epochs_no_imp = 0\n",
    "\n",
    "print(f'Training ResNet-50 on {DEVICE} ...\\n')\n",
    "print(f'{\"Epoch\":>6} | {\"Train Loss\":>10} | {\"Train Acc\":>9} | '\n",
    "      f'{\"Val Loss\":>8} | {\"Val Acc\":>8} | {\"LR\":>10}')\n",
    "print('-' * 65)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_loss, train_acc = run_epoch(model, train_loader, criterion, optimizer)\n",
    "    val_loss,   val_acc   = run_epoch(model, val_loader,   criterion)\n",
    "    scheduler.step()\n",
    "\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    marker     = ' *' if val_acc > best_val_acc else ''\n",
    "    print(f'{epoch:>6} | {train_loss:>10.4f} | {train_acc*100:>8.2f}% | '\n",
    "          f'{val_loss:>8.4f} | {val_acc*100:>7.2f}%{marker} | {current_lr:>10.2e}')\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc  = val_acc\n",
    "        epochs_no_imp = 0\n",
    "        torch.save({'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'val_acc': best_val_acc},\n",
    "                   CKPT_DIR / 'best_model.pth')\n",
    "    else:\n",
    "        epochs_no_imp += 1\n",
    "        if epochs_no_imp >= PATIENCE:\n",
    "            print(f'\\nEarly stopping at epoch {epoch} (no improvement for {PATIENCE} epochs).')\n",
    "            break\n",
    "\n",
    "print(f'\\nBest validation accuracy: {best_val_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ba4562",
   "metadata": {},
   "source": [
    "## 7. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0d22f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(x, history['train_loss'], label='Train', color='steelblue')\n",
    "ax1.plot(x, history['val_loss'],   label='Validation', color='coral')\n",
    "ax1.set_title('Cross-Entropy Loss', fontweight='bold')\n",
    "ax1.set_xlabel('Epoch'); ax1.set_ylabel('Loss')\n",
    "ax1.legend(); ax1.grid(alpha=0.3)\n",
    "\n",
    "ax2.plot(x, [a*100 for a in history['train_acc']], label='Train', color='steelblue')\n",
    "ax2.plot(x, [a*100 for a in history['val_acc']],   label='Validation', color='coral')\n",
    "ax2.set_title('Classification Accuracy (%)', fontweight='bold')\n",
    "ax2.set_xlabel('Epoch'); ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.legend(); ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Best val acc: {best_val_acc*100:.2f}%  |  '\n",
    "      f'Final train acc: {history[\"train_acc\"][-1]*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a032c3c",
   "metadata": {},
   "source": [
    "## 8. Evaluation on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d089891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Load best checkpoint ──────────────────────────────────────────────────────\n",
    "ckpt = torch.load(CKPT_DIR / 'best_model.pth', map_location=DEVICE)\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "print(f'Loaded checkpoint from epoch {ckpt[\"epoch\"]} (val acc: {ckpt[\"val_acc\"]*100:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c32b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        outputs = model(images.to(DEVICE))\n",
    "        all_preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "all_preds  = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "print(f'Validation Accuracy: {(all_preds == all_labels).mean()*100:.2f}%\\n')\n",
    "print(classification_report(\n",
    "    all_labels, all_preds,\n",
    "    target_names=[n.replace('_', ' ').title() for n in CLASS_NAMES],\n",
    "    digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfc6f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Normalised Confusion Matrix ───────────────────────────────────────────────\n",
    "cm      = confusion_matrix(all_labels, all_preds)\n",
    "cm_norm = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "display_names = [n.replace('_', ' ').title() for n in CLASS_NAMES]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 7))\n",
    "im = ax.imshow(cm_norm, cmap='Blues', vmin=0, vmax=1)\n",
    "fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "ticks = np.arange(NUM_CLASSES)\n",
    "ax.set_xticks(ticks); ax.set_xticklabels(display_names, rotation=35, ha='right', fontsize=9)\n",
    "ax.set_yticks(ticks); ax.set_yticklabels(display_names, fontsize=9)\n",
    "\n",
    "for i in range(NUM_CLASSES):\n",
    "    for j in range(NUM_CLASSES):\n",
    "        colour = 'white' if cm_norm[i, j] > 0.5 else 'black'\n",
    "        ax.text(j, i, f'{cm_norm[i, j]:.2f}\\n({cm[i, j]})',\n",
    "                ha='center', va='center', fontsize=8, color=colour)\n",
    "\n",
    "ax.set_title('Normalised Confusion Matrix — Validation Set', fontweight='bold')\n",
    "ax.set_xlabel('Predicted Class'); ax.set_ylabel('True Class')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7acb19",
   "metadata": {},
   "source": [
    "## 9. Test Set Evaluation & Submission Generation\n",
    "\n",
    "The held-out test split (15% of data) is used to simulate the competition leaderboard evaluation.\n",
    "A `submission.csv` is produced in the Kaggle format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d0be94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_preds, test_labels = [], []\n",
    "n_batches = len(test_loader)\n",
    "\n",
    "print(f'Running inference on {len(test_ds)} test images ...')\n",
    "with torch.no_grad():\n",
    "    for i, (images, labels) in enumerate(test_loader, 1):\n",
    "        outputs = model(images.to(DEVICE))\n",
    "        test_preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "        test_labels.extend(labels.numpy())\n",
    "        print_progress_bar(i, n_batches)\n",
    "\n",
    "test_preds  = np.array(test_preds)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "test_accuracy = (test_preds == test_labels).mean()\n",
    "print(f'\\nTest Accuracy: {test_accuracy*100:.2f}%\\n')\n",
    "print(classification_report(\n",
    "    test_labels, test_preds,\n",
    "    target_names=[n.replace('_', ' ').title() for n in CLASS_NAMES],\n",
    "    digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f831ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Generate submission CSV in Kaggle format ──────────────────────────────────\n",
    "IDX_TO_CLASS = {i: name for name, i in CLASS_TO_IDX.items()}\n",
    "pred_labels  = [IDX_TO_CLASS[p] for p in test_preds]\n",
    "image_ids    = [f'test_{i:05d}.jpg' for i in range(len(test_preds))]\n",
    "\n",
    "submission = pd.DataFrame({'id': image_ids, 'label': pred_labels})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print('submission.csv saved.')\n",
    "print(f'Shape: {submission.shape}')\n",
    "print('\\nPrediction distribution:')\n",
    "print(submission['label'].value_counts().to_string())\n",
    "print('\\nFirst 5 rows:')\n",
    "print(submission.head(5).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61069c0",
   "metadata": {},
   "source": [
    "## 10. Test-Time Augmentation (TTA)\n",
    "\n",
    "Averages softmax probabilities across three augmented views of each test image.  \n",
    "Typically yields a 0.5–2% accuracy improvement over single-pass inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6db00f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TTA_TRANSFORMS = [\n",
    "    eval_transform,   # original\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(p=1.0),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "    ]),\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomVerticalFlip(p=1.0),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "    ]),\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "tta_probs = None\n",
    "\n",
    "for t_idx, tta_tf in enumerate(TTA_TRANSFORMS):\n",
    "    tta_ds     = PatternNetDataset(hf_dataset, test_indices, PN_TO_COMP, OTHERS_LABEL, tta_tf)\n",
    "    tta_loader = DataLoader(tta_ds, batch_size=BATCH_SIZE,\n",
    "                            shuffle=False, num_workers=NUM_WORKERS)\n",
    "    probs_list = []\n",
    "\n",
    "    print(f'TTA pass {t_idx + 1}/{len(TTA_TRANSFORMS)} ...')\n",
    "    with torch.no_grad():\n",
    "        for i, (images, _) in enumerate(tta_loader, 1):\n",
    "            logits = model(images.to(DEVICE))\n",
    "            probs_list.extend(torch.softmax(logits, dim=1).cpu().numpy())\n",
    "            print_progress_bar(i, len(tta_loader))\n",
    "\n",
    "    tta_probs = np.array(probs_list) if tta_probs is None else tta_probs + np.array(probs_list)\n",
    "\n",
    "tta_preds       = tta_probs.argmax(axis=1)\n",
    "tta_pred_labels = [IDX_TO_CLASS[p] for p in tta_preds]\n",
    "tta_accuracy    = (tta_preds == test_labels).mean()\n",
    "\n",
    "tta_submission = pd.DataFrame({'id': image_ids, 'label': tta_pred_labels})\n",
    "tta_submission.to_csv('submission_tta.csv', index=False)\n",
    "\n",
    "print(f'\\nTTA Test Accuracy : {tta_accuracy*100:.2f}%')\n",
    "print(f'Standard Accuracy : {test_accuracy*100:.2f}%')\n",
    "print(f'Improvement       : {(tta_accuracy - test_accuracy)*100:+.2f}%')\n",
    "print('\\nsubmission_tta.csv saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecb943c",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "| Item | Value |\n",
    "|------|-------|\n",
    "| Data source | PatternNet via HuggingFace (`blanchon/PatternNet`) |\n",
    "| Architecture | ResNet-50 (ImageNet pre-trained, IMAGENET1K_V2) |\n",
    "| Classes | 7 (basketball court, beach, forest, railway, swimming pool, tennis court, others) |\n",
    "| Total images used | ~5,600 (6 × 800 target + 800 others) |\n",
    "| Split | 70% train / 15% val / 15% test (stratified) |\n",
    "| Input size | 224 × 224 |\n",
    "| Optimiser | AdamW (lr=3×10⁻⁴, wd=10⁻⁴) |\n",
    "| Scheduler | Cosine annealing with warm restarts (T₀=10) |\n",
    "| Loss | Cross-entropy + label smoothing (0.1) |\n",
    "| Augmentation | Random crop, H/V flip, rotation, colour jitter, random erasing |\n",
    "| TTA | 3 passes (original, H-flip, V-flip) |\n",
    "\n",
    "**Output files:**\n",
    "- `submission.csv` — standard single-pass inference on the test split\n",
    "- `submission_tta.csv` — test-time augmented predictions (recommended)\n",
    "\n",
    "**Possible further improvements:**\n",
    "- MixUp / CutMix augmentation during training\n",
    "- EfficientNet-B4 or ViT-B/16 for higher capacity\n",
    "- Weighted loss if class imbalance is significant\n",
    "- Ensemble of multiple ResNet and EfficientNet variants"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
