{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNR 638: Machine Learning for Remote Sensing\n",
    "\n",
    "**Task:** Multi-class image classification of remotely sensed images  \n",
    "**Competition:** [gnr638-mls4rs-a1 on Kaggle](https://www.kaggle.com/competitions/gnr638-mls4rs-a1)  \n",
    "**Architecture:** ResNet-50 with ImageNet pre-training (transfer learning)  \n",
    "**Classes (7):** Basketball Court, Beach, Forest, Railway, Tennis Court, Water Pool, Others\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **Accept competition rules:** Visit [kaggle.com/competitions/gnr638-mls4rs-a1](https://www.kaggle.com/competitions/gnr638-mls4rs-a1) and click *Join Competition* / accept the rules before running the data download cell.\n",
    "2. **Kaggle authentication:** `kagglehub` will prompt for your Kaggle username and API key on first use, or read from `~/.kaggle/kaggle.json`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import shutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# ── Reproducibility ──────────────────────────────────────────────────────────\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'PyTorch {torch.__version__} | Device: {DEVICE}')\n",
    "if DEVICE.type == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)} | '\n",
    "          f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Download via kagglehub\n",
    "\n",
    "> **Important:** You must have accepted the competition rules on Kaggle before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "from kagglehub import KaggleApiError\n",
    "\n",
    "DATA_DIR = Path('data')\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "TRAIN_DIR = DATA_DIR / 'train'\n",
    "TEST_DIR  = DATA_DIR / 'test'\n",
    "\n",
    "print('Downloading competition data via kagglehub ...')\n",
    "print('Note: You will be prompted for Kaggle credentials if not already cached.\\n')\n",
    "\n",
    "try:\n",
    "    download_path = kagglehub.competition_download(\n",
    "        competition='gnr638-mls4rs-a1',\n",
    "        path=str(DATA_DIR)\n",
    "    )\n",
    "    print(f'Files downloaded to: {download_path}')\n",
    "except KaggleApiError as e:\n",
    "    print(f'[ERROR] {e}')\n",
    "    print('\\nTroubleshooting steps:')\n",
    "    print('  1. Visit https://www.kaggle.com/competitions/gnr638-mls4rs-a1')\n",
    "    print('  2. Click \"Join Competition\" and accept the rules.')\n",
    "    print('  3. Ensure your Kaggle API key is valid (~/.kaggle/kaggle.json).')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect what was downloaded\n",
    "print('Downloaded files:')\n",
    "for p in sorted(DATA_DIR.rglob('*')):\n",
    "    if p.is_file():\n",
    "        size_mb = p.stat().st_size / 1e6\n",
    "        print(f'  {p.relative_to(DATA_DIR)}  ({size_mb:.1f} MB)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Extract zips if present ───────────────────────────────────────────────────\n",
    "import zipfile\n",
    "\n",
    "for zip_path in sorted(DATA_DIR.glob('*.zip')):\n",
    "    print(f'Extracting {zip_path.name} ...')\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "        zf.extractall(DATA_DIR)\n",
    "    print(f'  Done.')\n",
    "\n",
    "# Show the directory structure after extraction\n",
    "print('\\nData directory structure:')\n",
    "for p in sorted(DATA_DIR.rglob('*')):\n",
    "    depth = len(p.relative_to(DATA_DIR).parts)\n",
    "    indent = '  ' * (depth - 1)\n",
    "    if p.is_dir():\n",
    "        n_files = sum(1 for _ in p.iterdir() if _.is_file())\n",
    "        print(f'{indent}{p.name}/  [{n_files} files]')\n",
    "    elif depth <= 3 and p.suffix in ('.csv', '.txt', '.json'):\n",
    "        print(f'{indent}{p.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Inspection\n",
    "\n",
    "Identify train / test folder layout and label mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Locate training images and classes ───────────────────────────────────────\n",
    "# The competition typically provides a folder-per-class structure for train\n",
    "# and a flat folder + CSV for test.\n",
    "\n",
    "# Try to auto-detect train directory\n",
    "candidates = [DATA_DIR / 'train', DATA_DIR / 'Train', DATA_DIR / 'training']\n",
    "TRAIN_DIR = next((c for c in candidates if c.is_dir()), None)\n",
    "if TRAIN_DIR is None:\n",
    "    # Fall back: any subdirectory containing class subdirectories\n",
    "    for d in DATA_DIR.iterdir():\n",
    "        if d.is_dir() and any(sd.is_dir() for sd in d.iterdir()):\n",
    "            TRAIN_DIR = d\n",
    "            break\n",
    "\n",
    "print(f'Train directory: {TRAIN_DIR}')\n",
    "\n",
    "CLASS_NAMES = sorted([d.name for d in TRAIN_DIR.iterdir() if d.is_dir()])\n",
    "CLASS_TO_IDX = {cls: i for i, cls in enumerate(CLASS_NAMES)}\n",
    "NUM_CLASSES  = len(CLASS_NAMES)\n",
    "\n",
    "print(f'Number of classes : {NUM_CLASSES}')\n",
    "print(f'Class names       : {CLASS_NAMES}')\n",
    "\n",
    "# Count images per class\n",
    "counts = {}\n",
    "for cls in CLASS_NAMES:\n",
    "    imgs = list((TRAIN_DIR / cls).glob('*'))\n",
    "    imgs = [i for i in imgs if i.suffix.lower() in ('.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff')]\n",
    "    counts[cls] = len(imgs)\n",
    "\n",
    "print('\\nImages per class:')\n",
    "total = 0\n",
    "for cls, n in counts.items():\n",
    "    print(f'  {cls:<20} {n}')\n",
    "    total += n\n",
    "print(f'  {\"TOTAL\":<20} {total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Locate test directory and sample submission ───────────────────────────────\n",
    "test_candidates = [DATA_DIR / 'test', DATA_DIR / 'Test', DATA_DIR / 'testing']\n",
    "TEST_DIR = next((c for c in test_candidates if c.is_dir()), None)\n",
    "print(f'Test directory: {TEST_DIR}')\n",
    "\n",
    "if TEST_DIR:\n",
    "    test_images = sorted(TEST_DIR.glob('*'))\n",
    "    test_images = [i for i in test_images if i.suffix.lower() in ('.jpg','.jpeg','.png','.bmp')]\n",
    "    print(f'Test images   : {len(test_images)}')\n",
    "\n",
    "# Check for sample submission CSV\n",
    "sample_csvs = list(DATA_DIR.glob('sample*.csv')) + list(DATA_DIR.glob('*.csv'))\n",
    "if sample_csvs:\n",
    "    print(f'\\nSample submission: {sample_csvs[0].name}')\n",
    "    print(pd.read_csv(sample_csvs[0]).head(5).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Class distribution bar chart ─────────────────────────────────────────────\n",
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "bars = ax.bar(counts.keys(), counts.values(), color='steelblue', edgecolor='white')\n",
    "ax.set_title('Training Set — Class Distribution', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Class', fontsize=11)\n",
    "ax.set_ylabel('Number of Images', fontsize=11)\n",
    "ax.tick_params(axis='x', rotation=30)\n",
    "for bar, val in zip(bars, counts.values()):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5,\n",
    "            str(val), ha='center', va='bottom', fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.savefig('class_distribution.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Class balance ratio (min/max): {min(counts.values())/max(counts.values()):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Sample images per class ───────────────────────────────────────────────────\n",
    "SAMPLES_PER_CLASS = 3\n",
    "\n",
    "fig, axes = plt.subplots(NUM_CLASSES, SAMPLES_PER_CLASS,\n",
    "                         figsize=(SAMPLES_PER_CLASS * 3, NUM_CLASSES * 3))\n",
    "fig.suptitle('Sample Training Images per Class', fontsize=13, fontweight='bold', y=1.01)\n",
    "\n",
    "for row, cls in enumerate(CLASS_NAMES):\n",
    "    cls_dir = TRAIN_DIR / cls\n",
    "    imgs = sorted(cls_dir.glob('*'))\n",
    "    imgs = [i for i in imgs if i.suffix.lower() in ('.jpg','.jpeg','.png','.bmp')]\n",
    "    samples = random.sample(imgs, min(SAMPLES_PER_CLASS, len(imgs)))\n",
    "    for col, img_path in enumerate(samples):\n",
    "        ax = axes[row][col] if NUM_CLASSES > 1 else axes[col]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        if col == 0:\n",
    "            ax.set_ylabel(cls, fontsize=9, rotation=0, labelpad=55, va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sample_images.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Image size statistics ─────────────────────────────────────────────────────\n",
    "widths, heights = [], []\n",
    "all_img_paths = []\n",
    "for cls in CLASS_NAMES:\n",
    "    cls_dir = TRAIN_DIR / cls\n",
    "    paths = [p for p in cls_dir.glob('*')\n",
    "             if p.suffix.lower() in ('.jpg','.jpeg','.png','.bmp')]\n",
    "    all_img_paths.extend(paths)\n",
    "    for p in paths:\n",
    "        with Image.open(p) as img:\n",
    "            w, h = img.size\n",
    "            widths.append(w)\n",
    "            heights.append(h)\n",
    "\n",
    "print(f'Image dimensions (W x H):')\n",
    "print(f'  Width  — min: {min(widths)}, max: {max(widths)}, mean: {np.mean(widths):.0f}')\n",
    "print(f'  Height — min: {min(heights)}, max: {max(heights)}, mean: {np.mean(heights):.0f}')\n",
    "print(f'  Total training images: {len(widths)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Augmentation & DataLoaders\n",
    "\n",
    "ImageNet normalisation statistics are used since ResNet-50 was pre-trained on ImageNet.  \n",
    "Training transforms include random flips, rotations, colour jitter, and random erasing for regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Hyperparameters ───────────────────────────────────────────────────────────\n",
    "IMG_SIZE    = 224       # ResNet-50 native input size\n",
    "BATCH_SIZE  = 32\n",
    "VAL_SPLIT   = 0.2      # 20% of training data used for validation\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE + 32, IMG_SIZE + 32)),   # Slightly larger for crop\n",
    "    transforms.RandomCrop(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.3),\n",
    "    transforms.RandomRotation(degrees=20),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.05),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "    transforms.RandomErasing(p=0.2, scale=(0.02, 0.1)),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "test_transform = val_transform\n",
    "\n",
    "print('Transforms defined.')\n",
    "print(f'  Input size : {IMG_SIZE}x{IMG_SIZE}')\n",
    "print(f'  Batch size : {BATCH_SIZE}')\n",
    "print(f'  Val split  : {VAL_SPLIT*100:.0f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoteSensingDataset(Dataset):\n",
    "    \"\"\"Dataset for folder-per-class remote sensing images.\"\"\"\n",
    "\n",
    "    IMG_EXTS = {'.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff'}\n",
    "\n",
    "    def __init__(self, root_dir, class_to_idx, transform=None):\n",
    "        self.root_dir     = Path(root_dir)\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.transform    = transform\n",
    "        self.samples      = []   # list of (path, label)\n",
    "\n",
    "        for cls, idx in class_to_idx.items():\n",
    "            cls_dir = self.root_dir / cls\n",
    "            if not cls_dir.is_dir():\n",
    "                continue\n",
    "            for img_path in sorted(cls_dir.iterdir()):\n",
    "                if img_path.suffix.lower() in self.IMG_EXTS:\n",
    "                    self.samples.append((img_path, idx))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    \"\"\"Dataset for flat test directory (no labels).\"\"\"\n",
    "\n",
    "    IMG_EXTS = {'.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff'}\n",
    "\n",
    "    def __init__(self, test_dir, transform=None):\n",
    "        self.test_dir  = Path(test_dir)\n",
    "        self.transform = transform\n",
    "        self.samples   = sorted(\n",
    "            p for p in self.test_dir.iterdir()\n",
    "            if p.suffix.lower() in self.IMG_EXTS\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.samples[idx]\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, path.name\n",
    "\n",
    "\n",
    "print('Dataset classes defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Build train / validation split ───────────────────────────────────────────\n",
    "full_dataset = RemoteSensingDataset(TRAIN_DIR, CLASS_TO_IDX, transform=None)\n",
    "n_total = len(full_dataset)\n",
    "n_val   = int(n_total * VAL_SPLIT)\n",
    "n_train = n_total - n_val\n",
    "\n",
    "# Split indices (reproducible)\n",
    "generator = torch.Generator().manual_seed(SEED)\n",
    "train_subset, val_subset = random_split(full_dataset, [n_train, n_val], generator=generator)\n",
    "\n",
    "# Apply transforms via wrapper\n",
    "class TransformedSubset(Dataset):\n",
    "    def __init__(self, subset, transform):\n",
    "        self.subset    = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.subset[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "train_ds = TransformedSubset(train_subset, train_transform)\n",
    "val_ds   = TransformedSubset(val_subset,   val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,  num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE,\n",
    "                          shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(f'Total training images : {n_total}')\n",
    "print(f'  Train split         : {n_train}')\n",
    "print(f'  Validation split    : {n_val}')\n",
    "print(f'Train batches         : {len(train_loader)}')\n",
    "print(f'Val batches           : {len(val_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Test DataLoader (if test images are present) ──────────────────────────────\n",
    "test_loader = None\n",
    "if TEST_DIR and TEST_DIR.is_dir():\n",
    "    test_ds     = TestDataset(TEST_DIR, transform=test_transform)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE,\n",
    "                             shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f'Test images : {len(test_ds)}')\n",
    "else:\n",
    "    print('No test directory found — skipping test DataLoader.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model — ResNet-50 with Fine-Tuning\n",
    "\n",
    "Strategy: load ImageNet pre-trained weights, replace the final fully-connected layer for 7-class output, and fine-tune the full network with a lower learning rate for early layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet50(num_classes: int, pretrained: bool = True, freeze_backbone: bool = False):\n",
    "    \"\"\"Build ResNet-50 with a custom classification head.\"\"\"\n",
    "    weights = models.ResNet50_Weights.IMAGENET1K_V2 if pretrained else None\n",
    "    model   = models.resnet50(weights=weights)\n",
    "\n",
    "    if freeze_backbone:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # Replace final FC layer\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(p=0.4),\n",
    "        nn.Linear(in_features, num_classes)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_resnet50(NUM_CLASSES, pretrained=True, freeze_backbone=False)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "total_params     = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total parameters     : {total_params:,}')\n",
    "print(f'Trainable parameters : {trainable_params:,}')\n",
    "print(f'Model head           : {model.fc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training\n",
    "\n",
    "- **Loss:** Cross-entropy  \n",
    "- **Optimiser:** AdamW with weight decay  \n",
    "- **Scheduler:** Cosine annealing with warm restarts  \n",
    "- **Early stopping:** Halts training if validation accuracy does not improve for `PATIENCE` epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Training hyperparameters ──────────────────────────────────────────────────\n",
    "NUM_EPOCHS    = 30\n",
    "LR            = 3e-4\n",
    "WEIGHT_DECAY  = 1e-4\n",
    "PATIENCE      = 7       # Early stopping patience\n",
    "CHECKPOINT    = Path('checkpoints')\n",
    "CHECKPOINT.mkdir(exist_ok=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1)\n",
    "\n",
    "print(f'Epochs       : {NUM_EPOCHS}')\n",
    "print(f'Learning rate: {LR}')\n",
    "print(f'Weight decay : {WEIGHT_DECAY}')\n",
    "print(f'Patience     : {PATIENCE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress_bar(current, total, length=40):\n",
    "    \"\"\"Print a simple progress bar that updates in place.\"\"\"\n",
    "    progress = current / total\n",
    "    filled   = int(length * progress)\n",
    "    bar      = '#' * filled + '.' * (length - filled)\n",
    "    print(f'\\rProgress: [{bar}] {current}/{total} ({progress*100:.1f}%)',\n",
    "          end='', flush=True)\n",
    "    if current == total:\n",
    "        print()\n",
    "\n",
    "\n",
    "def run_epoch(model, loader, criterion, optimizer=None, device=DEVICE):\n",
    "    \"\"\"Run one epoch; if optimizer is None, operate in evaluation mode.\"\"\"\n",
    "    training = optimizer is not None\n",
    "    model.train() if training else model.eval()\n",
    "\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    with torch.set_grad_enabled(training):\n",
    "        for batch_idx, (images, labels) in enumerate(loader):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss    = criterion(outputs, labels)\n",
    "\n",
    "            if training:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total   += images.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc  = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "print('Training utilities defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "best_val_acc  = 0.0\n",
    "epochs_no_imp = 0\n",
    "\n",
    "print(f'Training ResNet-50 for up to {NUM_EPOCHS} epochs on {DEVICE} ...\\n')\n",
    "print(f'{\"Epoch\":>6} | {\"Train Loss\":>10} | {\"Train Acc\":>9} | '\n",
    "      f'{\"Val Loss\":>8} | {\"Val Acc\":>8} | {\"LR\":>10}')\n",
    "print('-' * 65)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_loss, train_acc = run_epoch(model, train_loader, criterion, optimizer)\n",
    "    val_loss,   val_acc   = run_epoch(model, val_loader,   criterion)\n",
    "    scheduler.step()\n",
    "\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    marker     = ' *' if val_acc > best_val_acc else ''\n",
    "\n",
    "    print(f'{epoch:>6} | {train_loss:>10.4f} | {train_acc*100:>8.2f}% | '\n",
    "          f'{val_loss:>8.4f} | {val_acc*100:>7.2f}%{marker} | {current_lr:>10.2e}')\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        epochs_no_imp = 0\n",
    "        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(),\n",
    "                    'val_acc': best_val_acc},\n",
    "                   CHECKPOINT / 'best_model.pth')\n",
    "    else:\n",
    "        epochs_no_imp += 1\n",
    "        if epochs_no_imp >= PATIENCE:\n",
    "            print(f'\\nEarly stopping triggered at epoch {epoch} (no improvement for {PATIENCE} epochs).')\n",
    "            break\n",
    "\n",
    "print(f'\\nBest validation accuracy: {best_val_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_ran = len(history['train_loss'])\n",
    "x = range(1, epochs_ran + 1)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(x, history['train_loss'], label='Train', color='steelblue')\n",
    "ax1.plot(x, history['val_loss'],   label='Validation', color='coral')\n",
    "ax1.set_title('Cross-Entropy Loss', fontweight='bold')\n",
    "ax1.set_xlabel('Epoch'); ax1.set_ylabel('Loss')\n",
    "ax1.legend(); ax1.grid(alpha=0.3)\n",
    "\n",
    "ax2.plot(x, [a*100 for a in history['train_acc']], label='Train', color='steelblue')\n",
    "ax2.plot(x, [a*100 for a in history['val_acc']],   label='Validation', color='coral')\n",
    "ax2.set_title('Classification Accuracy (%)', fontweight='bold')\n",
    "ax2.set_xlabel('Epoch'); ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.legend(); ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Final epoch {epochs_ran}: '\n",
    "      f'train acc = {history[\"train_acc\"][-1]*100:.2f}%, '\n",
    "      f'val acc = {history[\"val_acc\"][-1]*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Load best checkpoint ──────────────────────────────────────────────────────\n",
    "checkpoint = torch.load(CHECKPOINT / 'best_model.pth', map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f'Loaded checkpoint from epoch {checkpoint[\"epoch\"]} '\n",
    "      f'(val acc: {checkpoint[\"val_acc\"]*100:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Collect predictions on validation set ────────────────────────────────────\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images = images.to(DEVICE)\n",
    "        outputs = model(images)\n",
    "        preds   = outputs.argmax(dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "all_preds  = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "val_accuracy = (all_preds == all_labels).mean()\n",
    "print(f'Validation Accuracy: {val_accuracy*100:.2f}%\\n')\n",
    "print(classification_report(all_labels, all_preds, target_names=CLASS_NAMES, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Confusion Matrix ──────────────────────────────────────────────────────────\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "cm_norm = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 7))\n",
    "im = ax.imshow(cm_norm, interpolation='nearest', cmap='Blues', vmin=0, vmax=1)\n",
    "fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "tick_marks = np.arange(NUM_CLASSES)\n",
    "ax.set_xticks(tick_marks); ax.set_xticklabels(CLASS_NAMES, rotation=35, ha='right', fontsize=9)\n",
    "ax.set_yticks(tick_marks); ax.set_yticklabels(CLASS_NAMES, fontsize=9)\n",
    "\n",
    "thresh = 0.5\n",
    "for i in range(NUM_CLASSES):\n",
    "    for j in range(NUM_CLASSES):\n",
    "        colour = 'white' if cm_norm[i, j] > thresh else 'black'\n",
    "        ax.text(j, i, f'{cm_norm[i, j]:.2f}\\n({cm[i, j]})',\n",
    "                ha='center', va='center', fontsize=8, color=colour)\n",
    "\n",
    "ax.set_title('Normalised Confusion Matrix (Validation Set)', fontweight='bold')\n",
    "ax.set_xlabel('Predicted Class'); ax.set_ylabel('True Class')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Set Inference & Submission Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_loader is None:\n",
    "    print('No test loader — skipping inference.')\n",
    "else:\n",
    "    model.eval()\n",
    "    filenames, predictions = [], []\n",
    "    n_batches = len(test_loader)\n",
    "\n",
    "    print(f'Running inference on {len(test_ds)} test images ...')\n",
    "    with torch.no_grad():\n",
    "        for i, (images, fnames) in enumerate(test_loader, 1):\n",
    "            images  = images.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            preds   = outputs.argmax(dim=1).cpu().numpy()\n",
    "            filenames.extend(fnames)\n",
    "            predictions.extend(preds)\n",
    "            print_progress_bar(i, n_batches)\n",
    "\n",
    "    IDX_TO_CLASS = {v: k for k, v in CLASS_TO_IDX.items()}\n",
    "    pred_labels  = [IDX_TO_CLASS[p] for p in predictions]\n",
    "\n",
    "    submission = pd.DataFrame({'id': filenames, 'label': pred_labels})\n",
    "\n",
    "    # Match Kaggle sample submission format if available\n",
    "    if sample_csvs:\n",
    "        sample_df = pd.read_csv(sample_csvs[0])\n",
    "        id_col    = sample_df.columns[0]\n",
    "        label_col = sample_df.columns[1]\n",
    "        submission.columns = [id_col, label_col]\n",
    "\n",
    "    submission_path = 'submission.csv'\n",
    "    submission.to_csv(submission_path, index=False)\n",
    "\n",
    "    print(f'\\nSubmission saved to: {submission_path}')\n",
    "    print(f'Shape: {submission.shape}')\n",
    "    print('\\nPrediction distribution:')\n",
    "    print(submission.iloc[:, 1].value_counts().to_string())\n",
    "    print('\\nFirst 5 rows:')\n",
    "    print(submission.head(5).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test-Time Augmentation (TTA) — Optional Refinement\n",
    "\n",
    "TTA averages predictions over multiple augmented views of each test image, typically improving accuracy by 0.5–2%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_loader is None:\n",
    "    print('No test loader — skipping TTA.')\n",
    "else:\n",
    "    TTA_TRANSFORMS = [\n",
    "        val_transform,  # original\n",
    "        transforms.Compose([\n",
    "            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            transforms.RandomHorizontalFlip(p=1.0),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "        ]),\n",
    "        transforms.Compose([\n",
    "            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            transforms.RandomVerticalFlip(p=1.0),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "        ]),\n",
    "    ]\n",
    "\n",
    "    model.eval()\n",
    "    tta_probs   = None\n",
    "    tta_fnames  = None\n",
    "\n",
    "    for t_idx, tta_tf in enumerate(TTA_TRANSFORMS):\n",
    "        tta_ds     = TestDataset(TEST_DIR, transform=tta_tf)\n",
    "        tta_loader = DataLoader(tta_ds, batch_size=BATCH_SIZE,\n",
    "                                shuffle=False, num_workers=NUM_WORKERS)\n",
    "        probs_list, fname_list = [], []\n",
    "\n",
    "        print(f'TTA pass {t_idx + 1}/{len(TTA_TRANSFORMS)} ...')\n",
    "        with torch.no_grad():\n",
    "            for i, (images, fnames) in enumerate(tta_loader, 1):\n",
    "                images = images.to(DEVICE)\n",
    "                logits = model(images)\n",
    "                probs  = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "                probs_list.extend(probs)\n",
    "                fname_list.extend(fnames)\n",
    "                print_progress_bar(i, len(tta_loader))\n",
    "\n",
    "        if tta_probs is None:\n",
    "            tta_probs  = np.array(probs_list)\n",
    "            tta_fnames = fname_list\n",
    "        else:\n",
    "            tta_probs += np.array(probs_list)\n",
    "\n",
    "    tta_preds       = tta_probs.argmax(axis=1)\n",
    "    IDX_TO_CLASS    = {v: k for k, v in CLASS_TO_IDX.items()}\n",
    "    tta_pred_labels = [IDX_TO_CLASS[p] for p in tta_preds]\n",
    "\n",
    "    tta_submission = pd.DataFrame({'id': tta_fnames, 'label': tta_pred_labels})\n",
    "    if sample_csvs:\n",
    "        sample_df = pd.read_csv(sample_csvs[0])\n",
    "        tta_submission.columns = [sample_df.columns[0], sample_df.columns[1]]\n",
    "\n",
    "    tta_submission.to_csv('submission_tta.csv', index=False)\n",
    "    print('\\nTTA submission saved to: submission_tta.csv')\n",
    "    print('Prediction distribution (TTA):')\n",
    "    print(tta_submission.iloc[:, 1].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "| Item | Value |\n",
    "|------|-------|\n",
    "| Architecture | ResNet-50 (ImageNet pre-trained) |\n",
    "| Classes | 7 (basketball court, beach, forest, railway, tennis court, water pool, others) |\n",
    "| Input size | 224 x 224 |\n",
    "| Optimiser | AdamW (lr=3e-4, wd=1e-4) |\n",
    "| Scheduler | Cosine annealing with warm restarts (T_0=10) |\n",
    "| Loss | Cross-entropy with label smoothing (0.1) |\n",
    "| Augmentation | Random crop, flip, rotation, colour jitter, random erasing |\n",
    "| TTA passes | 3 (original, H-flip, V-flip) |\n",
    "\n",
    "**Submission files:**\n",
    "- `submission.csv` — standard inference\n",
    "- `submission_tta.csv` — test-time augmented (recommended for leaderboard)\n",
    "\n",
    "**Possible further improvements:**\n",
    "- Increase training data via additional augmentation (MixUp, CutMix)\n",
    "- Use EfficientNet-B4 or ViT-B/16 for higher capacity\n",
    "- Apply class-weighted loss if class imbalance is significant\n",
    "- Ensemble multiple ResNet variants"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
